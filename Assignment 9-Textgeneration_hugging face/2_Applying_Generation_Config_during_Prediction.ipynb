{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary classes from the transformers library\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
      ],
      "metadata": {
        "id": "FeafybYJRWph"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GPT-2 model for text generation\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "# Load the pre-trained GPT-2 tokenizer with space cleanup enabled\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", clean_up_tokenization_spaces=True)"
      ],
      "metadata": {
        "id": "qqLI9IcBRYbD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define generation configuration\n",
        "generation_config = GenerationConfig(\n",
        "    max_length=100,          # Maximum length of the generated text\n",
        "    num_beams=5,             # Number of beams for beam search; higher values improve output quality but increase computation\n",
        "    temperature=0.7,         # Controls the randomness of generation: lower values make the output more deterministic\n",
        "    top_k=50,                # Limits the sampling pool to the top k most likely next tokens\n",
        "    top_p=0.9,               # Nucleus sampling: considers tokens with cumulative probability mass of top_p\n",
        "    repetition_penalty=1.2,  # Penalizes repeated tokens to reduce redundancy in generated text\n",
        "    do_sample=True           # Enables sampling; if False, generates text using beam search\n",
        ")"
      ],
      "metadata": {
        "id": "Qg_joiF2RaGU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input text prompt\n",
        "input_text = \"In a distant future, humanity has discovered\"\n",
        "\n",
        "# Tokenize the input text and convert it to token IDs\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "# `return_tensors=\"pt\"` ensures the tokenized output is returned as a PyTorch tensor,\n",
        "# which is required for model input. `input_ids` holds the tensor of token IDs for the input text."
      ],
      "metadata": {
        "id": "M_XfH0MLRaBr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text based on the input with the custom configuration\n",
        "output = model.generate(input_ids, **generation_config.to_dict())\n",
        "# `input_ids` is passed to the model to generate text based on the input prompt.\n",
        "# `generation_config.to_dict()` converts the configuration to a dictionary and unpacks it as keyword arguments\n",
        "# to control the text generation process according to the specified settings."
      ],
      "metadata": {
        "id": "ILL_TSvBRg0r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated token IDs back into human-readable text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "# `output[0]` accesses the generated token IDs from the model's output.\n",
        "# `tokenizer.decode()` converts these token IDs back into text.\n",
        "# `skip_special_tokens=True` ensures that special tokens (like padding or end-of-sequence tokens) are excluded from the final output.\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)\n",
        "# Display the decoded and generated text to the user."
      ],
      "metadata": {
        "id": "VWNo977I4M72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4804843-953c-4a9c-c339-37a0c81554d9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a distant future, humanity has discovered a way to harness the power of the sun to create life on Earth. But what if we could also harness the power of the moon to create life on Mars?\n",
            "\n",
            "NASA's Mars Science Laboratory (MSL) is working with NASA's Jet Propulsion Laboratory (JPL) and the European Space Agency (ESA) to develop a solar-powered spacecraft capable of carrying astronauts to Mars.\n",
            "\n",
            "The mission will be launched from Cape Canaveral Air Force Station\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qUefWvzQfyI"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}